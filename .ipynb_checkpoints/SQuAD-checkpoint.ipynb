{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbd10bf7-4643-4135-a738-ddd72e328893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as layers\n",
    "import transformers\n",
    "import json\n",
    "import tokenizers\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dee82a-a458-4109-8f9f-18cca3b7429b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Download and Load URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7abaed-b024-4c22-883c-9ba3cd6fb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
    "test_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "\n",
    "train_load = keras.utils.get_file(fname='squad_train.json',\n",
    "                                 origin=train_url)\n",
    "test_load = keras.utils.get_file(fname='squad_test.json',\n",
    "                                origin=test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45469e74-2ed8-4bed-906c-5acb1b15057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_load) as f:\n",
    "    train_data = json.load(f)['data']\n",
    "    \n",
    "with open(test_load) as f:\n",
    "    test_data = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5dc054-9094-4e50-9aa7-35c2d591f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure:\n",
    "# articles --> 'paragraph' in article --> [qas,context] in article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9dcfb-f85e-4635-94b8-7d8cd1da8412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaf6d135-d4e0-4f9e-a980-2f32e19e4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformer we need\n",
    "# input_ids\n",
    "# token_type_ids\n",
    "# attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e30bf1-9e5d-472f-aad3-588e9d5c0589",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write Functions to Process Input Data\n",
    "* Make input_ids\n",
    "    * These are inputs in a form that BERT can process.\n",
    "    * Input should be in form [CLS] + Question + [SEP] + Context.\n",
    "    * This should then be padded and tokenized to get input_ids.\n",
    "* Token_type_ids\n",
    "    * Lets BERT know to what segment each part of the input corresponds.\n",
    "    * [CLS] through [SEP] are 0, the context is 1.\n",
    "* Attention_mask\n",
    "    * Prevent BERT from performing attention on padding tokens.\n",
    "    * Padding tokens are 0, everything else is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3ddfe-c66d-4a7d-8729-b3729c99d1cb",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e4dbbd-e5cf-453d-9fc6-fcfc362fb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer('bert-base-uncased-vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c51d34-6d93-441c-b2ff-8071c1304a66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce540014-b906-4a75-a68a-c5f692bb7bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a question and context, both in string form.\n",
    "# Return a list of input_ids\n",
    "def make_input_ids(question, context):\n",
    "    processed_question = question\n",
    "    encoded_question = tokenizer.encode(processed_question)\n",
    "    encoded_context = tokenizer.encode(context)\n",
    "    input_ids = encoded_question.ids + encoded_context.ids\n",
    "    return input_ids, len(encoded_question), len(encoded_context.ids), encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be583d-fde6-4754-865e-2663a90f5e9b",
   "metadata": {},
   "source": [
    "## Make Token Type IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "973974b6-a3d5-4938-aaa1-20497ba93c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the length of the question portion of the input_ids and\n",
    "# the full input_ids length.\n",
    "# Returns a token_type_id\n",
    "def make_token_id(question_length, context_length):\n",
    "    token_ids = [0] * question_length + [1] * context_length\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f65bd060-301b-436a-953d-a029c5228673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 11 [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[101, 2129, 2024, 2017, 1029, 102, 101, 1045, 2572, 2307, 102]\n"
     ]
    }
   ],
   "source": [
    "# Example of input_ids and token_ids\n",
    "ids, question_length, context_length, _ = make_input_ids('how are you?', 'I am great')\n",
    "\n",
    "token_id = make_token_id(question_length, context_length)\n",
    "\n",
    "print(question_length, len(token_id), token_id)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8dd0e-e8c8-4022-896b-f715ac30b4d2",
   "metadata": {},
   "source": [
    "## Pad sequences and Create Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb524452-8299-47b3-a96d-b9db471698a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes input_ids, token_ids, and a maximum length\n",
    "# Returns the padded input sequences and an attention mask\n",
    "def pad_and_mask_sequences(input_ids, token_ids, max_length):\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * padding_length\n",
    "    input_ids += [0] * padding_length\n",
    "    token_ids += [0] * padding_length\n",
    "    return attention_mask, token_ids, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d505a9-9b8e-47b3-a345-23d128bac4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of padding with max_length = 20\n",
    "attn, tok, id = pad_and_mask_sequences(ids, token_id, 20)\n",
    "len(attn), len(tok), len(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b940ae-64ed-4f7f-bd8f-3003f989c807",
   "metadata": {},
   "source": [
    "# Process Targets\n",
    "Targets are in the form of (start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7465e2bb-cb2c-429d-ae89-a8dd4e6195ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 4), (5, 8), (9, 13), (0, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.encode('dogs and cats')\n",
    "a.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce17f009-91e8-414a-aeb9-6ac2474b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an answer dictionary and an encoded context\n",
    "# Returns a start_index and end_index of the answer in the\n",
    "# tokenized context.\n",
    "\n",
    "def process_answers(answer, encoded_context):\n",
    "    # Find the start\n",
    "    start_index = answer['answer_start']\n",
    "    end_index = start_index + len(answer['text'])\n",
    "    \n",
    "    # Recover original context string by stripping start/end tokens\n",
    "    original_context = ' '.join(encoded_context.tokens[1:-1])\n",
    "    \n",
    "    # Create a of the context with answer characters as 1\n",
    "    answer_mask = [0] * len(original_context)\n",
    "    for index in range(start_index, end_index):\n",
    "        answer_mask[index] = 1\n",
    "        \n",
    "    # Use mask to find index of starting and ending tokens in the\n",
    "    # encoded answer.\n",
    "    # Offsets returns a tuple giving the starting and ending index\n",
    "    # of each token in the tokenized context.\n",
    "    answer_tokens = []\n",
    "    for index, (start, end) in enumerate(encoded_context.offsets):\n",
    "        if 1 in answer_mask[start:end]:\n",
    "            answer_tokens.append(index)\n",
    "    \n",
    "    start_index = answer_tokens[0]\n",
    "    end_index = answer_tokens[-1]\n",
    "    \n",
    "    return start_index, end_index         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d41ec-e1df-4361-8a6e-154ecf9e952c",
   "metadata": {},
   "source": [
    "## Put Preprocessing Functions Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc09722-dfc9-4f4f-bca3-4069b4bb6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(question, answer, context, max_length):\n",
    "    input_ids, question_length, context_length, encoded_context = make_input_ids(question, context)\n",
    "    token_type_ids = make_token_id(question_length, context_length)\n",
    "    attention_mask, token_type_ids, input_ids = pad_and_mask_sequences(input_ids, token_type_ids, max_length)\n",
    "    start_index, end_index = process_answers(answer, encoded_context)\n",
    "    return attention_mask, token_type_ids, input_ids, start_index, end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66236b-8798-4b30-aeec-30428806b72e",
   "metadata": {},
   "source": [
    "# Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce5206d4-704e-4a5d-9a67-37d62642dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, max_length):\n",
    "    x_output = {'attention_masks' : [],\n",
    "           'type_ids' : [],\n",
    "           'input_ids' : []}\n",
    "    y_output ={'start_index' : [],\n",
    "           'end_index' : []}\n",
    "    for sample in tqdm(data):\n",
    "        for paragraph in sample['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                # Only add to output if the question has an answer\n",
    "                if len(qa['answers']) > 0:\n",
    "                    answer = qa['answers'][0]\n",
    "                    attention_mask, token_type_ids, input_ids, start_index, end_index = process_text(question, answer, context, max_length)\n",
    "                    \n",
    "                    # Only keep sequences below max_length\n",
    "                    if len(input_ids) <= max_length:\n",
    "                        x_processing_outputs = attention_mask, token_type_ids, input_ids\n",
    "                        y_processing_outputs = start_index, end_index\n",
    "                        for index, key in enumerate(x_output.keys()):\n",
    "                            x_output[key].append(x_processing_outputs[index])\n",
    "\n",
    "                        for index, key in enumerate(y_output.keys()):\n",
    "                            y_output[key].append(y_processing_outputs[index])\n",
    "        \n",
    "    return x_output, y_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d532c63-d7b9-4b89-a7f0-e862b19a8cfc",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "For the sake of convenience, the below function checks to see if the processed data is already stored in a json file before performing any data processing. If it is already stored, the stored data is loaded instead of processing any data. If the data is not stored, it is saved in a json file after it is processed. This also performs some conversions to numpy arrays, since those are not json serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c842c0e5-8c66-46d6-9757-f724e571c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if data is saved to disk. If not, processes the \n",
    "# downloaded json files and saves the processed data to the disk.\n",
    "# Returns xtrain, ytrain always. Also returns xtest, ytest if load_test is set to true\n",
    "def load_data(maxlength, load_test=False):\n",
    "    if os.path.exists(f'xtrain_squad{maxlength}.json') and os.path.exists(f'ytrain_squad{maxlength}.json'):\n",
    "        print('Train data found on disk!')\n",
    "        with open(f'xtrain_squad{maxlength}.json', 'r') as f:\n",
    "            xtrain = json.load(f)\n",
    "        with open(f'ytrain_squad{maxlength}.json', 'r') as f:\n",
    "            ytrain = json.load(f)\n",
    "    else:\n",
    "        xtrain, ytrain = process_data(train_data, maxlength)\n",
    "        print('Writing xtrain')\n",
    "        with open(f'xtrain_squad{maxlength}.json', 'w') as f:\n",
    "            json.dump(xtrain, f)\n",
    "        print('Writing ytrain')\n",
    "        with open(f'ytrain_squad{maxlength}.json', 'w') as f:\n",
    "            json.dump(ytrain, f)\n",
    "    print('performing numpy conversion')      \n",
    "    for key in xtrain.keys():\n",
    "        xtrain[key] = np.asarray(xtrain[key], dtype=object).astype('float32')\n",
    "    for key in ytrain.keys():\n",
    "            output = np.array([[index] for index in ytrain[key]])\n",
    "            ytrain[key] = np.asarray(output)        \n",
    "            \n",
    "    if load_test:\n",
    "        if os.path.exists(f'xtest_squad{maxlength}.json') and os.path.exists(f'ytest_squad{maxlength}.json'):\n",
    "            print('Test data found on disk!')\n",
    "            with open(f'xtest_squad{maxlength}.json', 'r') as f:\n",
    "                xtest = json.load(f)\n",
    "            with open(f'ytest_squad{maxlength}.json', 'r') as f:\n",
    "                ytest = json.load(f)      \n",
    "        else:\n",
    "            xtest, ytest = process_data(test_data, maxlength)\n",
    "            print('Writing xtest')\n",
    "            with open(f'xtest_squad{maxlength}.json', 'w') as f:\n",
    "                json.dump(xtest, f)\n",
    "            print('Writing ytest')\n",
    "            with open(f'ytest_squad{maxlength}.json', 'w') as f:\n",
    "                json.dump(ytest, f)\n",
    "\n",
    "        print('performing numpy conversion')      \n",
    "        for key in xtest.keys():\n",
    "            xtest[key] = np.asarray(xtest[key], dtype=object).astype('float32')\n",
    "        for key in ytest.keys():\n",
    "            output = np.array([[index] for index in ytest[key]])\n",
    "            ytest[key] = np.asarray(output)\n",
    "                            \n",
    "        return xtrain, ytrain, xtest, ytest\n",
    "    else:\n",
    "        return xtrain, ytrain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fe93b61-2f06-4f3e-92c2-20cd0dffa58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data found on disk!\n",
      "performing numpy conversion\n",
      "Test data found on disk!\n",
      "performing numpy conversion\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain, xtest, ytest = load_data(128, load_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e377b15-f6fb-4d98-8fb1-064808f7963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 128)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = 5000\n",
    "data = [xtrain, ytrain, xtest, ytest]\n",
    "for dataset in data:\n",
    "    for key in dataset:\n",
    "        dataset[key] = dataset[key][:data_len]\n",
    "xtrain['input_ids'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5940-dd28-406e-bb86-12f113ed517d",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c7bf2-2315-42b8-ad5b-fe34c7eba4b6",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2c7b30cd-a73a-4c4d-9108-85591675eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength=128\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b53feb-9647-4aae-b9ad-0b3f234adc89",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba1fb608-ba67-4562-b36c-bbd7d0b11b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " type_ids (InputLayer)          [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'type_ids[0][0]']               \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128, 1)       768         ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 128)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,008\n",
      "Trainable params: 109,483,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "input_ids = layers.Input(shape=(maxlength,), \n",
    "                         name='input_ids',\n",
    "                         dtype=tf.int32)\n",
    "token_type_ids = layers.Input(shape=(maxlength),\n",
    "                              name='type_ids',\n",
    "                             dtype=tf.int32)\n",
    "attention_mask = layers.Input(shape=(maxlength), \n",
    "                              name='attention_masks',\n",
    "                             dtype=tf.int32)\n",
    "\n",
    "# Transformer\n",
    "bert = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# bert returns a tuple, the first item in which is the hidden state\n",
    "hidden_state = bert(input_ids=input_ids,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               attention_mask=attention_mask)[0]\n",
    "\n",
    "start = layers.Dense(1, use_bias=False)(hidden_state)\n",
    "start = layers.Flatten()(start)\n",
    "p_start = layers.Activation(keras.activations.softmax)(start)\n",
    "\n",
    "end = layers.Dense(1, use_bias=False)(hidden_state)\n",
    "end = layers.Flatten()(end)\n",
    "p_end = layers.Activation(keras.activations.softmax)(end)\n",
    "\n",
    "model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask],\n",
    "                   outputs=[end, end])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95e4ab07-133f-43e4-823f-b456b6a85b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scce = keras.losses.SparseCategoricalCrossentropy()\n",
    "opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer = opt, \n",
    "              loss=[scce,scce], \n",
    "              metrics=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "33cada09-0a49-4312-9aeb-ccde1ab48820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      " 1/79 [..............................] - ETA: 2:06:47 - loss: 31.6867 - flatten_1_loss: 15.8157 - flatten_1_1_loss: 15.8710 - flatten_1_Accuracy: 0.0000e+00 - flatten_1_1_Accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14552/2019680949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.fit(xtrain,\n\u001b[0m\u001b[0;32m      2\u001b[0m          \u001b[1;33m[\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'end_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m          batch_size=batch_size)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(xtrain,\n",
    "         [ytrain['start_index'],ytrain['end_index']],\n",
    "         epochs=1,\n",
    "         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b5960-6b11-4b27-95a4-b8b0668a5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain['start_index'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03df66-ab7b-43c3-a54c-2a1509ed7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
