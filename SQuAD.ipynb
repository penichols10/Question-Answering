{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd10bf7-4643-4135-a738-ddd72e328893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as layers\n",
    "import transformers\n",
    "import json\n",
    "import tokenizers\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dee82a-a458-4109-8f9f-18cca3b7429b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Download and Load URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7abaed-b024-4c22-883c-9ba3cd6fb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
    "test_url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'\n",
    "vocab_url = 'https://huggingface.co/distilbert-base-uncased/blob/main/vocab.txt'\n",
    "\n",
    "train_load = keras.utils.get_file(fname='squad_train.json',\n",
    "                                 origin=train_url)\n",
    "test_load = keras.utils.get_file(fname='squad_test.json',\n",
    "                                origin=test_url)\n",
    "vocab_load = keras.utils.get_file(fname='distilbert-base-uncased-vocab.txt',\n",
    "                                 origin=vocab_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45469e74-2ed8-4bed-906c-5acb1b15057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_load) as f:\n",
    "    train_data = json.load(f)['data']\n",
    "    \n",
    "with open(test_load) as f:\n",
    "    test_data = json.load(f)['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5dc054-9094-4e50-9aa7-35c2d591f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure:\n",
    "# articles --> 'paragraph' in article --> [qas,context] in article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9dcfb-f85e-4635-94b8-7d8cd1da8412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf6d135-d4e0-4f9e-a980-2f32e19e4580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transformer we need\n",
    "# input_ids\n",
    "# token_type_ids\n",
    "# attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e30bf1-9e5d-472f-aad3-588e9d5c0589",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Write Functions to Process Input Data\n",
    "* Make input_ids\n",
    "    * These are inputs in a form that BERT can process.\n",
    "    * Input should be in form [CLS] + Question + [SEP] + Context.\n",
    "    * This should then be padded and tokenized to get input_ids.\n",
    "* Token_type_ids\n",
    "    * Lets BERT know to what segment each part of the input corresponds.\n",
    "    * [CLS] through [SEP] are 0, the context is 1.\n",
    "* Attention_mask\n",
    "    * Prevent BERT from performing attention on padding tokens.\n",
    "    * Padding tokens are 0, everything else is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3ddfe-c66d-4a7d-8729-b3729c99d1cb",
   "metadata": {},
   "source": [
    "## Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e4dbbd-e5cf-453d-9fc6-fcfc362fb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = tokenizers.BertWordPieceTokenizer('distilbert-base-uncased-vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c51d34-6d93-441c-b2ff-8071c1304a66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce540014-b906-4a75-a68a-c5f692bb7bca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Takes a question and context, both in string form.\n",
    "# Return a list of input_ids\n",
    "def make_input_ids(question, context):\n",
    "    processed_question = question\n",
    "    encoded_question = tokenizer.encode(processed_question)\n",
    "    encoded_context = tokenizer.encode(context)\n",
    "    input_ids = encoded_question.ids + encoded_context.ids\n",
    "    return input_ids, len(encoded_question), len(encoded_context.ids), encoded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be583d-fde6-4754-865e-2663a90f5e9b",
   "metadata": {},
   "source": [
    "## Make Token Type IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973974b6-a3d5-4938-aaa1-20497ba93c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the length of the question portion of the input_ids and\n",
    "# the full input_ids length.\n",
    "# Returns a token_type_id\n",
    "def make_token_id(question_length, context_length):\n",
    "    token_ids = [0] * question_length + [1] * context_length\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f65bd060-301b-436a-953d-a029c5228673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 11 [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "[101, 2129, 2024, 2017, 1029, 102, 101, 1045, 2572, 2307, 102]\n"
     ]
    }
   ],
   "source": [
    "# Example of input_ids and token_ids\n",
    "ids, question_length, context_length, _ = make_input_ids('how are you?', 'I am great')\n",
    "\n",
    "token_id = make_token_id(question_length, context_length)\n",
    "\n",
    "print(question_length, len(token_id), token_id)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c8dd0e-e8c8-4022-896b-f715ac30b4d2",
   "metadata": {},
   "source": [
    "## Pad sequences and Create Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb524452-8299-47b3-a96d-b9db471698a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes input_ids, token_ids, and a maximum length\n",
    "# Returns the padded input sequences and an attention mask\n",
    "def pad_and_mask_sequences(input_ids, token_ids, max_length):\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    attention_mask = [1] * len(input_ids) + [0] * padding_length\n",
    "    input_ids += [0] * padding_length\n",
    "    token_ids += [0] * padding_length\n",
    "    return attention_mask, token_ids, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0d505a9-9b8e-47b3-a345-23d128bac4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of padding with max_length = 20\n",
    "attn, tok, id = pad_and_mask_sequences(ids, token_id, 20)\n",
    "len(attn), len(tok), len(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b940ae-64ed-4f7f-bd8f-3003f989c807",
   "metadata": {},
   "source": [
    "# Process Targets\n",
    "Targets are in the form of (start_index, end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7465e2bb-cb2c-429d-ae89-a8dd4e6195ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0), (0, 4), (5, 8), (9, 13), (0, 0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.encode('dogs and cats')\n",
    "a.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce17f009-91e8-414a-aeb9-6ac2474b1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in an answer dictionary and an encoded context\n",
    "# Returns a start_index and end_index of the answer in the\n",
    "# tokenized context.\n",
    "\n",
    "def process_answers(answer, encoded_context):\n",
    "    # Find the start\n",
    "    start_index = answer['answer_start']\n",
    "    end_index = start_index + len(answer['text'])\n",
    "    \n",
    "    # Recover original context string by stripping start/end tokens\n",
    "    original_context = ' '.join(encoded_context.tokens[1:-1])\n",
    "    \n",
    "    # Create a of the context with answer characters as 1\n",
    "    answer_mask = [0] * len(original_context)\n",
    "    for index in range(start_index, end_index):\n",
    "        answer_mask[index] = 1\n",
    "        \n",
    "    # Use mask to find index of starting and ending tokens in the\n",
    "    # encoded answer.\n",
    "    # Offsets returns a tuple giving the starting and ending index\n",
    "    # of each token in the tokenized context.\n",
    "    answer_tokens = []\n",
    "    for index, (start, end) in enumerate(encoded_context.offsets):\n",
    "        if 1 in answer_mask[start:end]:\n",
    "            answer_tokens.append(index)\n",
    "    \n",
    "    start_index = answer_tokens[0]\n",
    "    end_index = answer_tokens[-1]\n",
    "    \n",
    "    return start_index, end_index         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68d41ec-e1df-4361-8a6e-154ecf9e952c",
   "metadata": {},
   "source": [
    "## Put Preprocessing Functions Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc09722-dfc9-4f4f-bca3-4069b4bb6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(question, answer, context, max_length):\n",
    "    input_ids, question_length, context_length, encoded_context = make_input_ids(question, context)\n",
    "    token_type_ids = make_token_id(question_length, context_length)\n",
    "    attention_mask, token_type_ids, input_ids = pad_and_mask_sequences(input_ids, token_type_ids, max_length)\n",
    "    start_index, end_index = process_answers(answer, encoded_context)\n",
    "    return attention_mask, token_type_ids, input_ids, start_index, end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66236b-8798-4b30-aeec-30428806b72e",
   "metadata": {},
   "source": [
    "# Preprocess the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce5206d4-704e-4a5d-9a67-37d62642dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, max_length):\n",
    "    x_output = {'attention_masks' : [],\n",
    "           'type_ids' : [],\n",
    "           'input_ids' : []}\n",
    "    y_output ={'start_index' : [],\n",
    "           'end_index' : []}\n",
    "    for sample in tqdm(data):\n",
    "        for paragraph in sample['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                # Only add to output if the question has an answer\n",
    "                if len(qa['answers']) > 0:\n",
    "                    answer = qa['answers'][0]\n",
    "                    attention_mask, token_type_ids, input_ids, start_index, end_index = process_text(question, answer, context, max_length)\n",
    "                    \n",
    "                    # Only keep sequences below max_length\n",
    "                    if len(input_ids) <= max_length:\n",
    "                        x_processing_outputs = attention_mask, token_type_ids, input_ids\n",
    "                        y_processing_outputs = start_index, end_index\n",
    "                        for index, key in enumerate(x_output.keys()):\n",
    "                            x_output[key].append(x_processing_outputs[index])\n",
    "\n",
    "                        for index, key in enumerate(y_output.keys()):\n",
    "                            y_output[key].append(y_processing_outputs[index])\n",
    "        \n",
    "    return x_output, y_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d532c63-d7b9-4b89-a7f0-e862b19a8cfc",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "For the sake of convenience, the below function checks to see if the processed data is already stored in a json file before performing any data processing. If it is already stored, the stored data is loaded instead of processing any data. If the data is not stored, it is saved in a json file after it is processed. This also performs some conversions to numpy arrays, since those are not json serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c842c0e5-8c66-46d6-9757-f724e571c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks to see if data is saved to disk. If not, processes the \n",
    "# downloaded json files and saves the processed data to the disk.\n",
    "# Returns xtrain, ytrain always. Also returns xtest, ytest if load_test is set to true\n",
    "def load_data(maxlength, load_test=False):\n",
    "    if os.path.exists(f'xtrain_squad{maxlength}.json') and os.path.exists(f'ytrain_squad{maxlength}.json'):\n",
    "        print('Train data found on disk!')\n",
    "        with open(f'xtrain_squad{maxlength}.json', 'r') as f:\n",
    "            xtrain = json.load(f)\n",
    "        with open(f'ytrain_squad{maxlength}.json', 'r') as f:\n",
    "            ytrain = json.load(f)\n",
    "    else:\n",
    "        xtrain, ytrain = process_data(train_data, maxlength)\n",
    "        print('Writing xtrain')\n",
    "        with open(f'xtrain_squad{maxlength}.json', 'w') as f:\n",
    "            json.dump(xtrain, f)\n",
    "        print('Writing ytrain')\n",
    "        with open(f'ytrain_squad{maxlength}.json', 'w') as f:\n",
    "            json.dump(ytrain, f)\n",
    "    print('performing numpy conversion')      \n",
    "    for key in xtrain.keys():\n",
    "        xtrain[key] = np.asarray(xtrain[key], dtype=object).astype('float32')\n",
    "    for key in ytrain.keys():\n",
    "            output = np.array([[index] for index in ytrain[key]])\n",
    "            ytrain[key] = np.asarray(output)        \n",
    "            \n",
    "    if load_test:\n",
    "        if os.path.exists(f'xtest_squad{maxlength}.json') and os.path.exists(f'ytest_squad{maxlength}.json'):\n",
    "            print('Test data found on disk!')\n",
    "            with open(f'xtest_squad{maxlength}.json', 'r') as f:\n",
    "                xtest = json.load(f)\n",
    "            with open(f'ytest_squad{maxlength}.json', 'r') as f:\n",
    "                ytest = json.load(f)      \n",
    "        else:\n",
    "            xtest, ytest = process_data(test_data, maxlength)\n",
    "            print('Writing xtest')\n",
    "            with open(f'xtest_squad{maxlength}.json', 'w') as f:\n",
    "                json.dump(xtest, f)\n",
    "            print('Writing ytest')\n",
    "            with open(f'ytest_squad{maxlength}.json', 'w') as f:\n",
    "                json.dump(ytest, f)\n",
    "\n",
    "        print('performing numpy conversion')      \n",
    "        for key in xtest.keys():\n",
    "            xtest[key] = np.asarray(xtest[key], dtype=object).astype('float32')\n",
    "        for key in ytest.keys():\n",
    "            output = np.array([[index] for index in ytest[key]])\n",
    "            ytest[key] = np.asarray(output)\n",
    "                            \n",
    "        return xtrain, ytrain, xtest, ytest\n",
    "    else:\n",
    "        return xtrain, ytrain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7b30cd-a73a-4c4d-9108-85591675eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength=256\n",
    "batch_size= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe93b61-2f06-4f3e-92c2-20cd0dffa58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data found on disk!\n",
      "performing numpy conversion\n",
      "Test data found on disk!\n",
      "performing numpy conversion\n"
     ]
    }
   ],
   "source": [
    "xtrain, ytrain, xtest, ytest = load_data(maxlength, load_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e377b15-f6fb-4d98-8fb1-064808f7963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78318, 256)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_len = 1000\n",
    "data = [xtrain, ytrain, xtest, ytest]\n",
    "for dataset in data:\n",
    "    for key in dataset:\n",
    "        dataset[key] = dataset[key][:]\n",
    "xtrain['input_ids'].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f5940-dd28-406e-bb86-12f113ed517d",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18c7bf2-2315-42b8-ad5b-fe34c7eba4b6",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b53feb-9647-4aae-b9ad-0b3f234adc89",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba1fb608-ba67-4562-b36c-bbd7d0b11b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_ids[0][0]',              \n",
      " BertModel)                     ast_hidden_state=(N               'attention_masks[0][0]']        \n",
      "                                one, 256, 768),                                                   \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256, 1)       769         ['tf_distil_bert_model[0][0]']   \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 256, 1)       769         ['tf_distil_bert_model[0][0]']   \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 256)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " type_ids (InputLayer)          [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 256)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 256)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 66,364,418\n",
      "Trainable params: 66,364,418\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "input_ids = layers.Input(shape=(maxlength,), \n",
    "                         name='input_ids',\n",
    "                         dtype=tf.int32)\n",
    "token_type_ids = layers.Input(shape=(maxlength),\n",
    "                              name='type_ids',\n",
    "                             dtype=tf.int32)\n",
    "attention_mask = layers.Input(shape=(maxlength), \n",
    "                              name='attention_masks',\n",
    "                             dtype=tf.int32)\n",
    "\n",
    "# Transformer\n",
    "bert = transformers.TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# bert returns a tuple, the first item in which is the hidden state\n",
    "hidden_state = bert(input_ids=input_ids,\n",
    "                               #token_type_ids=token_type_ids,\n",
    "                               attention_mask=attention_mask)[0]\n",
    "\n",
    "start = layers.Dense(1)(hidden_state)\n",
    "start = layers.Flatten()(start)\n",
    "p_start = layers.Activation(keras.activations.softmax)(start)\n",
    "\n",
    "end = layers.Dense(1)(hidden_state)\n",
    "end = layers.Flatten()(end)\n",
    "p_end = layers.Activation(keras.activations.softmax)(end)\n",
    "\n",
    "model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask],\n",
    "                   outputs=[p_start, p_end])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95e4ab07-133f-43e4-823f-b456b6a85b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "scce = keras.losses.SparseCategoricalCrossentropy()\n",
    "opt = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.compile(optimizer = opt, \n",
    "              loss=[scce,scce], \n",
    "              metrics=['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33cada09-0a49-4312-9aeb-ccde1ab48820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3133/3133 [==============================] - 814s 259ms/step - loss: 6.6958 - activation_loss: 3.3081 - activation_1_loss: 3.3877 - activation_Accuracy: 0.1437 - activation_1_Accuracy: 0.1246 - val_loss: 5.0137 - val_activation_loss: 2.4894 - val_activation_1_loss: 2.5243 - val_activation_Accuracy: 0.2798 - val_activation_1_Accuracy: 0.2554\n",
      "Epoch 2/50\n",
      "3133/3133 [==============================] - 809s 258ms/step - loss: 4.2210 - activation_loss: 2.1155 - activation_1_loss: 2.1055 - activation_Accuracy: 0.3652 - activation_1_Accuracy: 0.3655 - val_loss: 3.9138 - val_activation_loss: 1.9942 - val_activation_1_loss: 1.9196 - val_activation_Accuracy: 0.4445 - val_activation_1_Accuracy: 0.4747\n",
      "Epoch 3/50\n",
      "3133/3133 [==============================] - 818s 261ms/step - loss: 3.1228 - activation_loss: 1.5831 - activation_1_loss: 1.5397 - activation_Accuracy: 0.5031 - activation_1_Accuracy: 0.5177 - val_loss: 3.7611 - val_activation_loss: 1.9292 - val_activation_1_loss: 1.8319 - val_activation_Accuracy: 0.4795 - val_activation_1_Accuracy: 0.5239\n",
      "Epoch 4/50\n",
      "3133/3133 [==============================] - 781s 249ms/step - loss: 2.4325 - activation_loss: 1.2539 - activation_1_loss: 1.1786 - activation_Accuracy: 0.5902 - activation_1_Accuracy: 0.6204 - val_loss: 3.7414 - val_activation_loss: 1.9260 - val_activation_1_loss: 1.8154 - val_activation_Accuracy: 0.5107 - val_activation_1_Accuracy: 0.5472\n",
      "Epoch 5/50\n",
      "3133/3133 [==============================] - 775s 247ms/step - loss: 1.9770 - activation_loss: 1.0287 - activation_1_loss: 0.9483 - activation_Accuracy: 0.6550 - activation_1_Accuracy: 0.6853 - val_loss: 3.7931 - val_activation_loss: 1.9653 - val_activation_1_loss: 1.8278 - val_activation_Accuracy: 0.5092 - val_activation_1_Accuracy: 0.5398\n",
      "Epoch 6/50\n",
      "3133/3133 [==============================] - 775s 247ms/step - loss: 1.6413 - activation_loss: 0.8625 - activation_1_loss: 0.7788 - activation_Accuracy: 0.7053 - activation_1_Accuracy: 0.7394 - val_loss: 3.9651 - val_activation_loss: 1.9979 - val_activation_1_loss: 1.9671 - val_activation_Accuracy: 0.5397 - val_activation_1_Accuracy: 0.5753\n",
      "Epoch 7/50\n",
      "3133/3133 [==============================] - 775s 248ms/step - loss: 1.3848 - activation_loss: 0.7299 - activation_1_loss: 0.6550 - activation_Accuracy: 0.7468 - activation_1_Accuracy: 0.7775 - val_loss: 4.3009 - val_activation_loss: 2.2496 - val_activation_1_loss: 2.0513 - val_activation_Accuracy: 0.5122 - val_activation_1_Accuracy: 0.5565\n",
      "Epoch 8/50\n",
      "3133/3133 [==============================] - 776s 248ms/step - loss: 1.1834 - activation_loss: 0.6343 - activation_1_loss: 0.5491 - activation_Accuracy: 0.7777 - activation_1_Accuracy: 0.8115 - val_loss: 4.7461 - val_activation_loss: 2.4353 - val_activation_1_loss: 2.3109 - val_activation_Accuracy: 0.5272 - val_activation_1_Accuracy: 0.5638\n",
      "Epoch 9/50\n",
      "3133/3133 [==============================] - 777s 248ms/step - loss: 1.0307 - activation_loss: 0.5495 - activation_1_loss: 0.4812 - activation_Accuracy: 0.8078 - activation_1_Accuracy: 0.8341 - val_loss: 4.5664 - val_activation_loss: 2.3217 - val_activation_1_loss: 2.2447 - val_activation_Accuracy: 0.5265 - val_activation_1_Accuracy: 0.5649\n",
      "Epoch 10/50\n",
      "3133/3133 [==============================] - 777s 248ms/step - loss: 0.9056 - activation_loss: 0.4803 - activation_1_loss: 0.4253 - activation_Accuracy: 0.8320 - activation_1_Accuracy: 0.8518 - val_loss: 4.7149 - val_activation_loss: 2.3840 - val_activation_1_loss: 2.3310 - val_activation_Accuracy: 0.5398 - val_activation_1_Accuracy: 0.5704\n",
      "Epoch 11/50\n",
      "3133/3133 [==============================] - 776s 248ms/step - loss: 0.8034 - activation_loss: 0.4269 - activation_1_loss: 0.3765 - activation_Accuracy: 0.8509 - activation_1_Accuracy: 0.8695 - val_loss: 5.0733 - val_activation_loss: 2.5830 - val_activation_1_loss: 2.4904 - val_activation_Accuracy: 0.5305 - val_activation_1_Accuracy: 0.5616\n",
      "Epoch 12/50\n",
      "3133/3133 [==============================] - 777s 248ms/step - loss: 0.7271 - activation_loss: 0.3852 - activation_1_loss: 0.3419 - activation_Accuracy: 0.8637 - activation_1_Accuracy: 0.8817 - val_loss: 5.1409 - val_activation_loss: 2.5869 - val_activation_1_loss: 2.5540 - val_activation_Accuracy: 0.5363 - val_activation_1_Accuracy: 0.5758\n",
      "Epoch 13/50\n",
      "3133/3133 [==============================] - 776s 248ms/step - loss: 0.6560 - activation_loss: 0.3476 - activation_1_loss: 0.3084 - activation_Accuracy: 0.8777 - activation_1_Accuracy: 0.8944 - val_loss: 5.3968 - val_activation_loss: 2.7424 - val_activation_1_loss: 2.6545 - val_activation_Accuracy: 0.5428 - val_activation_1_Accuracy: 0.5668\n",
      "Epoch 14/50\n",
      "3133/3133 [==============================] - 777s 248ms/step - loss: 0.6005 - activation_loss: 0.3166 - activation_1_loss: 0.2839 - activation_Accuracy: 0.8872 - activation_1_Accuracy: 0.9013 - val_loss: 5.2596 - val_activation_loss: 2.6471 - val_activation_1_loss: 2.6125 - val_activation_Accuracy: 0.5259 - val_activation_1_Accuracy: 0.5663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b024eff760>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain,\n",
    "         [ytrain['start_index'],ytrain['end_index']],\n",
    "          validation_split=0.2,\n",
    "          callbacks = [callback],\n",
    "         epochs=50,\n",
    "         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "719b5960-6b11-4b27-95a4-b8b0668a5b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain['start_index'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03df66-ab7b-43c3-a54c-2a1509ed7fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
